{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pt \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from spam.csv\n",
    "\n",
    "dataset = pd.read_csv('spam.csv' , usecols=[0,1], encoding ='ISO-8859-1')\n",
    "dataset.head()\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'\n",
      " 'Ok lar... Joking wif u oni...'\n",
      " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
      " 'U dun say so early hor... U c already then say...'\n",
      " \"Nah I don't think he goes to usf, he lives around here though\"\n",
      " \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\"\n",
      " 'Even my brother is not like to speak with me. They treat me like aids patent.'\n",
      " \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\"\n",
      " 'WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.'\n",
      " 'Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030']\n",
      "[0 0 1 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Removes the extra whitespace\n",
    "dataset['labels'] = dataset['labels'].str.strip()\n",
    "\n",
    "# Map the labels to binary values\n",
    "dataset['labels'] = dataset['labels'].map({'not spam': 0, 'spam': 1})\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "sentences = dataset['data'].values\n",
    "labels = dataset['labels'].values\n",
    "\n",
    "print(sentences[:10])\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sentences: ['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'\n",
      " 'Ok lar... Joking wif u oni...'\n",
      " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
      " 'U dun say so early hor... U c already then say...'\n",
      " \"Nah I don't think he goes to usf, he lives around here though\"]\n",
      "Training Labels: [0 0 1 0 0]\n",
      "Testing Sentences: ['That depends. How would you like to be treated? :)'\n",
      " 'Right on brah, see you later'\n",
      " 'Waiting in e car 4 my mum lor. U leh? Reach home already?'\n",
      " 'Your 2004 account for 07XXXXXXXXX shows 786 unredeemed points. To claim call 08719181259 Identifier code: XXXXX Expires 26.03.05'\n",
      " 'Do you want a new video handset? 750 anytime any network mins? Half Price Line Rental? Camcorder? Reply or call 08000930705 for delivery tomorrow']\n",
      "Testing Labels: [0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "training_size = int(0.7*len(sentences))\n",
    "\n",
    "training_sentences = sentences[:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[:training_size]\n",
    "testing_labels = labels[training_size:]\n",
    "\n",
    "print(\"Training Sentences:\" , training_sentences[:5])\n",
    "print(\"Training Labels:\", training_labels[:5])\n",
    "print(\"Testing Sentences:\" , testing_sentences[:5])\n",
    "print(\"Testing Labels:\" ,  testing_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 7439\n"
     ]
    }
   ],
   "source": [
    "# Tokenized the training\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 10000)\n",
    "\n",
    "# Build the vocabulary \n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "vocabulary = len(tokenizer.word_index)\n",
    "\n",
    "print(\"Number of unique words:\", vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sequences: [[54, 426, 3510, 711, 786, 640, 65, 8, 1138, 85, 121, 309, 1297, 131, 2360, 1024, 66, 59, 3511, 136], [46, 331, 2361, 562, 6, 1817], [50, 427, 8, 22, 4, 859, 934, 2, 170, 1513, 1025, 534, 1514, 1818, 253, 1819, 71, 1513, 2, 1820, 2, 347, 427, 490, 935, 72, 443, 185, 602, 388, 2362], [6, 254, 152, 25, 320, 3512, 6, 142, 148, 55, 152], [860, 1, 92, 99, 69, 428, 2, 1139, 69, 1821, 203, 102, 491]]\n",
      "Testing Sequences: [[19, 1225, 52, 172, 3, 58, 2, 29, 6154], [145, 18, 7353, 96, 3, 120], [244, 8, 131, 300, 44, 11, 775, 86, 6, 528, 371, 83, 148], [13, 1546, 356, 12, 3061, 383, 2420, 2421, 809, 2, 123, 17, 1046, 492, 6671, 1047, 732, 3145], [30, 3, 73, 4, 100, 440, 2492, 825, 826, 101, 444, 373, 350, 573, 305, 1213, 942, 88, 27, 17, 943, 12, 567, 150]]\n"
     ]
    }
   ],
   "source": [
    "# Create sequences of tokens which represents each sentence\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "\n",
    "print(\"Training Sequences:\" , train_sequences[:5])\n",
    "print(\"Testing Sequences:\" , test_sequences[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training sequences: 3900\n",
      "Length of training sequence: 189\n",
      "Shape of padded test sequences: (1672, 189)\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   54  426 3510  711  786  640   65    8 1138   85  121  309 1297\n",
      "  131 2360 1024   66   59 3511  136]\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences to have the same length to be passed into the RNN model\n",
    "\n",
    "padded_train = pad_sequences(train_sequences)\n",
    "\n",
    "no_of_sequences = padded_train.shape[0]\n",
    "length_of_sequence = padded_train.shape[1]\n",
    "\n",
    "padded_test = pad_sequences(test_sequences, maxlen = length_of_sequence)\n",
    "\n",
    "print(\"No. of training sequences:\", no_of_sequences)\n",
    "print(\"Length of training sequence:\", length_of_sequence)\n",
    "print(\"Shape of padded test sequences:\",  padded_test.shape)\n",
    "print(padded_train[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">189</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">148,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">189</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">189</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m189\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │       \u001b[38;5;34m148,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m189\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │         \u001b[38;5;34m2,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m189\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m16\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,976</span> (589.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m150,976\u001b[0m (589.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,976</span> (589.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,976\u001b[0m (589.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining the RNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer \n",
    "model.add(Input(shape=(length_of_sequence,)))\n",
    "\n",
    "# Defining embedding layer's output space dimension \n",
    "output_embed_dimension = 20\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(vocabulary + 1, output_embed_dimension)) \n",
    "\n",
    "# Defining LSTM layer's output space dimension (Also the number of neurons in the LSTM layer)\n",
    "LSTM_units = 15\n",
    "\n",
    "# LSTM layer\n",
    "model.add(LSTM(LSTM_units, return_sequences=True))\n",
    "\n",
    "model.add(Dropout((0.5)))\n",
    "\n",
    "# Pooling layer to reduce the dimensionality of the output to pass into the Sigmoid layer\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Dense layer with sigmoid activation function for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - accuracy: 0.8285 - loss: 0.4694 - val_accuracy: 0.8636 - val_loss: 0.3344\n",
      "Epoch 2/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.9343 - loss: 0.1921 - val_accuracy: 0.9809 - val_loss: 0.1915\n",
      "Epoch 3/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9877 - loss: 0.0713 - val_accuracy: 0.9815 - val_loss: 0.1426\n",
      "Epoch 4/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9929 - loss: 0.0427 - val_accuracy: 0.9868 - val_loss: 0.1219\n",
      "Epoch 5/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9941 - loss: 0.0317 - val_accuracy: 0.9868 - val_loss: 0.1075\n",
      "Epoch 6/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9971 - loss: 0.0196 - val_accuracy: 0.9892 - val_loss: 0.0908\n",
      "Epoch 7/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9991 - loss: 0.0133 - val_accuracy: 0.9892 - val_loss: 0.0828\n",
      "Epoch 8/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9988 - loss: 0.0123 - val_accuracy: 0.9898 - val_loss: 0.0777\n",
      "Epoch 9/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9995 - loss: 0.0081 - val_accuracy: 0.9904 - val_loss: 0.0730\n",
      "Epoch 10/10\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9997 - loss: 0.0066 - val_accuracy: 0.9904 - val_loss: 0.0688\n"
     ]
    }
   ],
   "source": [
    "# Training the model \n",
    "\n",
    "history = model.fit(padded_train, \n",
    "                    training_labels, \n",
    "                    epochs= 10, \n",
    "                    validation_data=(padded_test, testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
